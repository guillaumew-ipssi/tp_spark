{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dépendances\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install API Kaggle\n",
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Kaggle API \n",
    "import kaggle\n",
    "import zipfile\n",
    "\n",
    "#connect to API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "#download the file and unzip \n",
    "api.dataset_download_files('austinreese/craigslist-carstrucks-data', path='./data/vehicles/')\n",
    "\n",
    "with zipfile.ZipFile('./data/vehicles/craigslist-carstrucks-data.zip', 'r') as zipref:\n",
    "    zipref.extractall('./data/vehicles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du fichier\n",
    "#path = \"./data/vehicles/vehicles_min.csv\"\n",
    "path = \"./data/vehicles/vehicles.csv\"\n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentative de split d'un gros fichier en plusieurs fichiers\n",
    "\n",
    "#Chargement du fichier\n",
    "#path = \"./data/vehicles/vehicles\"\n",
    "#df = spark.read.format('csv').options(header=True).load(path)\n",
    "#df.count()\n",
    "\n",
    "#On récupère 20% du dataFrame initial\n",
    "#df_sample = df.sample(0.2)\n",
    "\n",
    "#Sur le DF récupéré, on le partionne en 50 partitions\n",
    "#df_aftercut = df_sample.coalesce(50)\n",
    "\n",
    "#On écrit un fichier .csv pour chaque partition\n",
    "#df_aftercut.write.format(\"csv\").option(\"header\", \"true\").save(\"./data/vehicles/vehicles_cut/\")\n",
    "\n",
    "#Nouveau path avec le DF partitionné\n",
    "#path_cut = \"./data/vehicles/vehicles_cut/*.csv\"\n",
    "#df_cut = spark.read.format('csv').options(header=True).load(path_cut)\n",
    "#df_cut.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |||\n",
    "# VERSION FICHIER 5000 LIGNES\n",
    "# ||| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "df.head()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les champs contenant des liens polluants la data\n",
    "cases = df.select('url','region_url','image_url')\n",
    "cases.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de certains champs inutiles\n",
    "df = df.drop('url')\n",
    "df = df.drop('region_url') \n",
    "df = df.drop('image_url')\n",
    "df = df.drop('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Présentation de la data\n",
    "\n",
    "path = \"./data/vehicles/vehicles_min.csv\"\n",
    "#path = \"./data/vehicles/vehicles.csv\"\n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "\n",
    "#Répartition des voitures en fonction des régions, avec représentation graphique\n",
    "df_region_name = df.select(\"region\").distinct().collect()\n",
    "df_region_values = df.groupBy(\"region\").agg({\"region\":\"count\"}).withColumnRenamed(\"COUNT(region)\", \"nb\")\n",
    "\n",
    "arr_region_name = np.array(df_region_name)\n",
    "arr_region_values = df_region_values.select(\"nb\").collect()\n",
    "\n",
    "values_matplot = np.array(arr_region_values)\n",
    "values_matplot = values_matplot.flatten()\n",
    "plt.pie(values_matplot, labels = arr_region_name, normalize = True)\n",
    "plt.legend(values_matplot, title=\"Region\", loc=\"center left\", bbox_to_anchor=(2, 0, 0.5, 1))\n",
    "\n",
    "df.groupBy(\"region\").agg({\"region\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/vehicles/vehicles_min.csv\"\n",
    "#path = \"./data/vehicles/vehicles.csv\"\n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "\n",
    "#Repartition des voitures en fonction du carburant, avec représentation graphique\n",
    "df_fuel_name = df.select(\"fuel\").distinct().na.fill('Undefined').collect()\n",
    "df_fuel_values = df.groupBy(\"fuel\").agg({\"fuel\":\"count\"}).withColumnRenamed(\"COUNT(fuel)\", \"nb\")\n",
    "\n",
    "arr_fuel_name = np.array(df_fuel_name)\n",
    "arr_fuel_values = df_fuel_values.select(\"nb\").collect()\n",
    "\n",
    "values_matplot = np.array(arr_fuel_values)\n",
    "values_matplot = values_matplot.flatten()\n",
    "\n",
    "\n",
    "plt.bar(arr_fuel_name.flatten(), values_matplot, width= 2, align='center')\n",
    "\n",
    "df.groupBy(\"fuel\").agg({\"fuel\": \"count\"}).where(F.col(\"fuel\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/vehicles/vehicles_min.csv\"\n",
    "#path = \"./data/vehicles/vehicles.csv\"\n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "\n",
    "#Répartition des voitures en fonction des types de véhicule, avec représentation graphique\n",
    "df_type_name = df.select(\"type\").distinct().collect()\n",
    "df_type_values = df.groupBy(\"type\").agg({\"type\":\"count\"}).withColumnRenamed(\"COUNT(type)\", \"nb\")\n",
    "\n",
    "arr_type_name = np.array(df_type_name)\n",
    "arr_type_values = df_type_values.select(\"nb\").collect()\n",
    "\n",
    "values_matplot = np.array(arr_type_values)\n",
    "values_matplot = values_matplot.flatten()\n",
    "plt.pie(values_matplot, labels = arr_type_name, normalize = True)\n",
    "plt.legend(values_matplot, title=\"Type\", loc=\"center left\", bbox_to_anchor=(2, 0, 0.5, 1))\n",
    "\n",
    "df.groupBy(\"type\").agg({\"type\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/vehicles/vehicles_min.csv\"\n",
    "#path = \"./data/vehicles/vehicles.csv\"\n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.filter(df[\"year\"] > \"2013.0\")\n",
    "\n",
    "df_avg = df.groupBy(\"year\").agg({\"price\":\"mean\"}).withColumnRenamed(\"COUNT(year)\", \"year\")\n",
    "df_price = df_avg.select(\"avg(price)\").orderBy(\"year\").collect()\n",
    "df_year = df_avg.select(\"year\").orderBy(\"year\").collect()\n",
    "\n",
    "arr_price = np.array(df_price) \n",
    "arr_year = np.array(df_year)\n",
    "\n",
    "plt.plot(arr_year.flatten(), arr_price.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
